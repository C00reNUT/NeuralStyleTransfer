{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram(tensor):\n",
    "    return torch.mm(tensor, tensor.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_loss(noise_img_gram, style_img_gram, N, M):\n",
    "    return torch.sum(torch.pow(noise_img_gram - style_img_gram, 2)).div((np.power(N*M*2, 2, dtype=np.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(image):\n",
    "    # shift one pixel and get difference (for both x and y direction)\n",
    "    loss = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n",
    "            torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the images\n",
    "cont_img = Image.open('./content_img_1.jpg')\n",
    "style_img = Image.open('./style_img_3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the transform\n",
    "transform = transforms.Compose([transforms.Resize((1024, 1024)),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                     [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tensor of the image\n",
    "content_image = transform(cont_img).unsqueeze(0).cuda()\n",
    "style_image = transform(style_img).unsqueeze(0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the VGG\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        \n",
    "        # load the vgg model's features\n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "    \n",
    "    def get_content_activations(self, x):\n",
    "        return self.vgg[:22](x)\n",
    "    \n",
    "    def get_style_activations(self, x):\n",
    "        # block1_conv1, block2_conv1, block3_conv1, block4_conv1, block5_conv1\n",
    "        return [self.vgg[:4](x)] + [self.vgg[:7](x)] + [self.vgg[:12](x)] + [self.vgg[:21](x)] + [self.vgg[:30](x)] \n",
    "#         return [self.vgg[:4](x)]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.vgg(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the network\n",
    "vgg = VGG().cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (vgg): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the MaxPool with the AvgPool layers\n",
    "for name, child in vgg.vgg.named_children():\n",
    "    if isinstance(child, nn.MaxPool2d):\n",
    "        vgg.vgg[int(name)] = nn.AvgPool2d(kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock the gradient\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the content activations of the content image and detach them from the graph\n",
    "content_activations = vgg.get_content_activations(content_image).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unroll the content activations\n",
    "content_F = content_activations.view(512, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the style activations of the style image\n",
    "style_activations = vgg.get_style_activations(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_activations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every layer in the style activations\n",
    "for i in range(len(style_activations)):\n",
    "    \n",
    "    # unroll the activations and detach them from the graph\n",
    "    style_activations[i] = style_activations[i].squeeze().view(style_activations[i].shape[1], -1).detach()\n",
    "\n",
    "# calculate the gram matrices of the style image\n",
    "gram_matrices = [gram(style_activations[i]) for i in range(len(style_activations))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_activations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_matrices[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the Gaussian noise\n",
    "noise = torch.randn(1, 3, 1024, 1024, device='cuda', requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the adam optimizer\n",
    "# pass the fearture map pixels to the optimnizer as parameters\n",
    "adam = optim.Adam(params=[noise], lr=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "# run the iteration\n",
    "for iteration in range(20000):\n",
    "    \n",
    "    # zero grad\n",
    "    adam.zero_grad()\n",
    "    \n",
    "    # get the content activations of the Gaussian noise\n",
    "    noise_content_activations = vgg.get_content_activations(noise)\n",
    "    \n",
    "    # unroll the feature maps of the noise\n",
    "    noise_content_F = noise_content_activations.view(512, -1)\n",
    "    \n",
    "    # calculate the content loss\n",
    "    content_loss = 1/2. * torch.sum(torch.pow(noise_content_F - content_F, 2))\n",
    "    \n",
    "    # get the style activations of the noise image\n",
    "    noise_style_activations = vgg.get_style_activations(noise)\n",
    "    \n",
    "    # for every layer\n",
    "    for i in range(len(noise_style_activations)):\n",
    "        \n",
    "        # unroll the the noise style activations\n",
    "        noise_style_activations[i] = noise_style_activations[i].squeeze().view(noise_style_activations[i].shape[1], -1)\n",
    "    \n",
    "    # calculate the noise gram matrices\n",
    "    noise_gram_matrices = [gram(noise_style_activations[i]) for i in range(len(noise_style_activations))]\n",
    "    \n",
    "    # calculate the total style loss\n",
    "    style_loss = 0\n",
    "    for i in range(len(style_activations)):\n",
    "        N, M = noise_style_activations[i].shape[0], noise_style_activations[i].shape[1]\n",
    "        style_loss += (gram_loss(noise_gram_matrices[i], gram_matrices[i], N, M) / 5.)\n",
    "\n",
    "    variation_loss = total_variation_loss(noise).cuda()\n",
    "        \n",
    "    style_loss = style_loss.cuda()\n",
    "    # try to reproduce the style\n",
    "#     total_loss = 10e6 * style_loss + 10e3 * variation_loss\n",
    "    total_loss = 10e-4 * content_loss + 10e6 * style_loss + 10e3 * variation_loss\n",
    "    \n",
    "    if iteration % 1000 == 0:\n",
    "        print(\"Iteration: {}, Content Loss: {:.3f}, Style Loss: {:.3f}, Var Loss: {:.3f}\".format(iteration, \n",
    "                                                                                                 10e-4 * content_loss.item(), \n",
    "                                                                                                 10e6 * style_loss.item(), \n",
    "                                                                                                 10e3 * variation_loss.item()))\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        save_image(noise.cpu().detach(), filename='./generated/iter_{}.png'.format(iteration))\n",
    "        \n",
    "    total_loss.backward()\n",
    "    adam.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noise.cpu().detach().squeeze().permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
